{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e337fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "from langchain_upstage import UpstageEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./assets/ewha/ewha.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new .pdf which contains only .txt information\n",
    "\n",
    "reader = PdfReader(input_path)\n",
    "writer = PdfWriter()\n",
    "\n",
    "exclude_pages = {19, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51}  \n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    if i not in exclude_pages:\n",
    "        writer.add_page(page)\n",
    "        \n",
    "with open(\"./assets/ewha/ewha_text.pdf\", \"wb\") as f:\n",
    "    writer.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new .pdf which contains only table information\n",
    "\n",
    "reader = PdfReader(input_path)\n",
    "writer = PdfWriter()\n",
    "\n",
    "keep_pages = [19, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]  \n",
    "\n",
    "for page_num in keep_pages:\n",
    "    if page_num < len(reader.pages):\n",
    "        writer.add_page(reader.pages[page_num])\n",
    "\n",
    "with open(\"./assets/ewha/ewha_table.pdf\", \"wb\") as f:\n",
    "    writer.write(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543e022",
   "metadata": {},
   "source": [
    "### .txt\n",
    "\n",
    "1. 장 단위로 쪼갬\n",
    "2. 수작업으로 각 장 내부의 조 단위로 쪼개어 chapters.txt 파일로 저장\n",
    "3. 각 장 내부에서 RecursiveTextSplitter(config: max_token=500, overlap=100)으로 쪼개어 ewha_chunk.jsonl 파일로 저장\n",
    "4. embedding 후 faiss vectorstore에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .txt만 청킹\n",
    "def extract_chapters_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF에서 장 단위로 텍스트를 추출하고 파싱\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: PDF 파일 경로\n",
    "    \n",
    "    Returns:\n",
    "        list: [{chapter_num, chapter_title, content}, ...]\n",
    "    \"\"\"\n",
    "    \n",
    "    full_text = \"\"\n",
    "    \n",
    "    # 1. extract text from .pdf\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text += text + \"\\n\"\n",
    "    \n",
    "    # 2. remove page number\n",
    "    lines = full_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if re.match(r'^[\\d\\-\\s]+$', line):\n",
    "            continue\n",
    "            \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    # 3. 장(Chapter) 단위로 분리\n",
    "    # 패턴: \"제N장 [제목]\" 또는 \"제N조[제목]\"\n",
    "    chapter_pattern = r'제(\\d+)장\\s+([^\\n]+)'\n",
    "    \n",
    "    chapters = []\n",
    "    matches = list(re.finditer(chapter_pattern, cleaned_text))\n",
    "    \n",
    "    for i, match in enumerate(matches):\n",
    "        chapter_num = match.group(1)\n",
    "        chapter_title = match.group(2).strip()\n",
    "        \n",
    "        start_pos = match.end()\n",
    "        \n",
    "        if i < len(matches) - 1:\n",
    "            end_pos = matches[i + 1].start()\n",
    "        else:\n",
    "            end_pos = len(cleaned_text)\n",
    "        \n",
    "        content = cleaned_text[start_pos:end_pos].strip()\n",
    "        \n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        content = re.sub(r'\\n\\s*\\n', '\\n', content)\n",
    "        \n",
    "        chapters.append({\n",
    "            'chapter_num': chapter_num,\n",
    "            'chapter_title': chapter_title,\n",
    "            'content': content\n",
    "        })\n",
    "    \n",
    "    return chapters\n",
    "\n",
    "\n",
    "def save_chapters_to_txt(chapters, output_path=\"./assets/ewha/chapters.txt\"):\n",
    "    \"\"\"\n",
    "    추출된 장들을 텍스트 파일로 저장\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chapter in chapters:\n",
    "            f.write(f\"제{chapter['chapter_num']}장: {chapter['chapter_title']}\\n\")\n",
    "            f.write(f\"{chapter['content']}\\n\\n\")\n",
    "    \n",
    "    print(f\"총 {len(chapters)}개 장이 {output_path}에 저장되었습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./assets/ewha/ewha_text.pdf\"\n",
    "    \n",
    "    chapters = extract_chapters_from_pdf(pdf_path)\n",
    "    \n",
    "    for chapter in chapters:\n",
    "        print(f\"\\n제{chapter['chapter_num']}장: {chapter['chapter_title']}\")\n",
    "        print(f\"내용 길이: {len(chapter['content'])}자\")\n",
    "        print(f\"내용 미리보기: {chapter['content'][:100]}...\")\n",
    "    \n",
    "    save_chapters_to_txt(chapters)\n",
    "    \n",
    "    print(f\"\\n총 {len(chapters)}개의 장이 추출되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af20afe",
   "metadata": {},
   "source": [
    "직접 ./assets/chapters.txt의 각 장 내부를 조 단위로 쪼갬. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37275735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 1) 장 기준 분리 함수\n",
    "# ---------------------------\n",
    "def split_by_chapter(text):\n",
    "    pattern = r\"(제\\d+장[:\\s]*[^\\n]*)\"\n",
    "    parts = re.split(pattern, text)\n",
    "\n",
    "    if not parts[0].startswith(\"제\"):\n",
    "        parts = parts[1:]\n",
    "\n",
    "    chapters = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        chapter_title = parts[i].strip()\n",
    "        chapter_body = parts[i+1].strip() if i+1 < len(parts) else \"\"\n",
    "        chapters.append((chapter_title, chapter_body))\n",
    "\n",
    "    return chapters\n",
    "\n",
    "# ---------------------------\n",
    "# 2) 청킹 함수\n",
    "# ---------------------------\n",
    "def chunk_text(text, max_char=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_char\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "# ---------------------------\n",
    "# 3) 장별 청킹\n",
    "# ---------------------------\n",
    "def chunk_by_chapter(text, max_char=500, overlap=100):\n",
    "    chapters = split_by_chapter(text)\n",
    "    result = []\n",
    "\n",
    "    for title, body in chapters:\n",
    "        chunks = chunk_text(body, max_char, overlap)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            result.append({\n",
    "                \"chapter\": title,\n",
    "                \"chunk_id\": idx,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) 실제 실행 + JSONL 저장\n",
    "# ---------------------------\n",
    "\n",
    "input_path = \"./assets/ewha/chapters.txt\"   \n",
    "output_path = \"./assets/ewha/ewha_chunk_text.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunks = chunk_by_chapter(text, max_char=500, overlap=100)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in chunks:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"저장 완료! → {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a08693",
   "metadata": {},
   "source": [
    "### table\n",
    "- Claude.ai으로 ewha_table.pdf를 텍스트 형태로 ewha_table.jsonl 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 및 출력 파일 경로\n",
    "input_file = './assets/ewha/ewha_table.jsonl'\n",
    "output_file = './assets/ewha/ewha_chunk_table.jsonl'\n",
    "\n",
    "# chunk_id 시작 번호\n",
    "chunk_id = 46\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    \n",
    "    for line in f_in:\n",
    "        data = json.loads(line.strip())\n",
    "        \n",
    "        table_type = data.get('table_type', '')\n",
    "        college = data.get('college', '')\n",
    "        content = data.get('content', '')\n",
    "        \n",
    "        if college:\n",
    "            text = f\"{table_type} {college} {content}\"\n",
    "        else:\n",
    "            text = f\"{table_type} {content}\"\n",
    "        \n",
    "        # ewha_chunk_text.jsonl과 구조 통합\n",
    "        new_data = {\n",
    "            'text': text,\n",
    "            'metadata': {\n",
    "                'chunk_id': chunk_id\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        f_out.write(json.dumps(new_data, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # chunk_id 증가\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"변환 완료! 총 {chunk_id - 46}개의 레코드가 변환되었습니다.\")\n",
    "print(f\"출력 파일: {output_file}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fca718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_save_dir = \"./faiss/faiss_ewha\"              \n",
    "\n",
    "# ---------------------------\n",
    "# 1) JSONL → Document 리스트 변환\n",
    "# ---------------------------\n",
    "\n",
    "docs = []\n",
    "\n",
    "# JSONL 파일 경로 리스트\n",
    "jsonl_files = [\n",
    "    \"./assets/ewha/ewha_chunk_text.jsonl\",\n",
    "    \"./assets/ewha/ewha_chunk_table.jsonl\"\n",
    "]\n",
    "\n",
    "# 각 파일을 순회하며 Document 생성\n",
    "for jsonl_path in jsonl_files:\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            text = item[\"text\"]\n",
    "            \n",
    "            metadata = {}\n",
    "            \n",
    "            if \"chunk_id\" in item:\n",
    "                metadata[\"chunk_id\"] = item[\"chunk_id\"]\n",
    "            elif \"metadata\" in item and \"chunk_id\" in item[\"metadata\"]:\n",
    "                metadata[\"chunk_id\"] = item[\"metadata\"][\"chunk_id\"]\n",
    "            \n",
    "            docs.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "print(f\"총 Document 개수: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstage_embedding_model =  UpstageEmbeddings(\n",
    "                            model=\"solar-embedding-1-large-passage\",\n",
    "                            api_key=UPSTAGE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) FAISS vectorstore 생성\n",
    "# ---------------------------\n",
    "docs = sorted(docs, key=lambda x: x.page_content) # 코드 추가\n",
    "db = FAISS.from_documents(docs, upstage_embedding_model)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) 저장\n",
    "# ---------------------------\n",
    "db.save_local(faiss_save_dir)\n",
    "\n",
    "print(f\"FAISS 저장 완료 → {faiss_save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
