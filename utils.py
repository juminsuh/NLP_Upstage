import os
import re
import pandas as pd
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain_community.retrievers import BM25Retriever

def load_api_key():
    load_dotenv('.env', override=True)
    UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
    return UPSTAGE_API_KEY

# def randomness_control():
#     # remove randomness
#     random.seed(42)
#     np.random.seed(42)
#     os.environ["PYTHONHASHSEED"] = str(42)
#     os.environ["CUDA_VISIBLE_DEVICES"] = ""
#     faiss.omp_set_num_threads(1)
    
def format_docs(docs):
    return '\n\n'.join(doc.page_content for doc in docs)
    
def extract_answer(response):
    """
    extracts the answer from the response using a regular expression.
    expected format: "[ANSWER]: (A) convolutional networks"

    if there are any answers formatted like the format, it returns None.
    """
    pattern = r"\[ANSWER\]:\s*\((A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|W|Z)\)"
    matches = re.findall(pattern, response)
    
    if matches:
        return matches[-1]  # return last matching
    else:
        return extract_again(response)

def extract_again(response):
    pattern = r"\b[A-J]\b"
    matches = re.findall(pattern, response)
    if matches:
        return matches[-1]  
    else:
        return None
    
def read_data(data_path):
    data = pd.read_csv(data_path)
    prompts = data['prompts']
    answers = data['answers']
    # returns two lists: prompts and answers
    return prompts, answers

def read_data_for_final(data_path):
    data = pd.read_csv(data_path)
    prompts = data['question']
    answers = data['your_answer']
    # returns two lists: prompts and answers
    return prompts, answers

def route(llm, prompt):
    
    prompt_template = PromptTemplate.from_template(
        """
        You are an expert text classifier.
        Below is an instruction that describes a task.
        Write a reponse that appropriately completes the instruction.
        
        ### Execution protocol:
        - If a text is written in Korean, classify a text into 1. Ewha Womans University rules, regardless of its content.

        ### Instruction:
        Classify the following question into ONE of six categories:
        1. Ewha Womans University rules
        2. Law
        3. History
        4. Philosophy
        5. Psychology
        6. Business

        ### Input:
        {question}

        ### Example:

        QUESTION1) ë³µìˆ˜ì „ê³µì„ ì´ìˆ˜í•˜ëŠ” í•™ìƒì˜ ì¬í•™ì—°í•œì€ ìµœëŒ€ ëª‡ ë…„ì¸ê°€ìš”?
        (A) 6ë…„
        (B) 7ë…„
        (C) 8ë…„
        (D) 9ë…„

        Answer: 1

        --------------------

        QUESTION31) During the manic phase of a bipolar disorder, individuals are most likely to experience
        (A) extreme fatigue
        (B) high self-esteem
        (C) memory loss
        (D) intense fear and anxiety

        Answer: 5

        ### Response:
        Return ONLY a single number from 1 to 6.
        **No Explanation**.
        You MUST follow the valid output format.

        ### Valid Output Examples:
        3
        1
        6
        """
    )
    chain = prompt_template | llm

    response = chain.invoke({"question": prompt})
    
    return response.content

def parse_question_and_choices(prompt):

    # extract question+options
    question_match = re.search(r'QUESTION\d+\)\s*(.*?)(?=\([A-Z]\))', prompt, re.DOTALL)

    if not question_match:
        raise ValueError("ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    question = question_match.group(1).strip()

    # extract options
    choice_pattern = r'\(([A-Z])\)\s*(.*?)(?=\([A-Z]\)|$)'
    choices_matches = re.finditer(choice_pattern, prompt, re.DOTALL)

    choices = []
    for match in choices_matches:
        label = match.group(1)
        text = match.group(2).strip()
        choices.append({
            'label': label,
            'text': text
        })

    return question, choices

def ewha_context(query_embedding, search_type, k, lambda_mult, fetch_k, prompt):
    # load db
    db = FAISS.load_local("./faiss_vectorstore/ewha",
                            query_embedding,
                            allow_dangerous_deserialization=True)

    # retriever
    retriever = db.as_retriever(search_type=search_type,
                                search_kwargs={'k': k, 'lambda_mult': lambda_mult, 'fetch_k': fetch_k})
    
    # parse question & choice
    q, choice_list = parse_question_and_choices(prompt)

    context = ""
    # implement rag for each choice
    for _, choice in enumerate(choice_list):
        qa = f"{q}\n{choice}"
        docs = retriever.invoke(qa)
        for doc in docs:
            if doc.page_content not in context: # prevent duplication 
                context += f'\n\n{doc.page_content}'
    
    return context
    
def mmlu_context(routed_result, embedding_model, search_type, k, lambda_mult, fetch_k, prompt):
    # ---- Load FAISS DB ----
    db = FAISS.load_local(
        f"./faiss_vectorstore/{routed_result}",
        embedding_model,
        allow_dangerous_deserialization=True
    )

    # ---- Dense Retriever (FAISS) ----
    dense_retriever = db.as_retriever(
        search_type=search_type,
        search_kwargs={'k': k, 'lambda_mult': lambda_mult, 'fetch_k': fetch_k}
    )

    # ---- BM25 Retriever ----
    documents = list(db.docstore._dict.values())
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = k
    
    # ---- Retrieve Relevant Docs ----
    dense_docs = dense_retriever.invoke(prompt)
    sparse_docs = bm25_retriever.invoke(prompt)

    # ---- Merge (Hybrid Retrieval) ----
    merged = {}

    for doc in dense_docs:
        merged[doc.page_content] = doc

    for doc in sparse_docs:
        if doc.page_content not in merged:
            merged[doc.page_content] = doc

    docs = list(merged.values())[:k]  
    context = format_docs(docs=docs)
    
    return context    

def ewha_rag(prompt, context, llm):

    prompt_template = ChatPromptTemplate.from_messages([
    ("system",
    """
    ë‹¹ì‹ ì€ ì´í™”ì—¬ìëŒ€í•™êµ í•™ì¹™ì„ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê³  ìˆëŠ” ê´€ë¦¬ìì…ë‹ˆë‹¤.

    ì‹¤í–‰ í”„ë¡œí† ì½œì„ ì •í™•í•˜ê²Œ ìˆ™ì§€í•˜ì„¸ìš”. ëª¨ë“  ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì •í™•í•˜ê²Œ ë”°ë¼ ì˜¬ë°”ë¥¸ ìµœì¢… ë‹µì„ ì¶œë ¥í•˜ì„¸ìš”.
    ëª¨ë“  ë‹¨ê³„ì˜ ê³¼ì •ì„ ë§¤ìš° êµ¬ì²´ì ì´ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.

    **ì‹¤í–‰ í”„ë¡œí† ì½œ**:
    - ì ˆëŒ€ ë§¥ë½ì— ëª…ì‹œë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì„ ì„ì˜ë¡œ ì „ì œí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ. ë‹¨, ë§¥ë½ì˜ ì˜ë¯¸ë¥¼ 100% ë³´ì¡´í•˜ëŠ” í•œë„ ë‚´ì—ì„œ ë§¥ë½ì„ ìœ ì—°í•˜ê²Œ ì ìš© ë° ì‚¬ê³  ê°€ëŠ¥. 
    - ë§¥ë½ì— ëª…í™•íˆ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ ì„ ì§€ë¥¼ í’€ì´í•  ê²ƒ.
    - ë§¥ë½ì„ ì½ê³  ê°„ë‹¨í•œ ì‚¬ì¹™ ì—°ì‚° (ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ)ì„ ì‹¤ìˆ˜ì—†ì´ ìˆ˜í–‰í•  ê²ƒ.
    - ì¡°í•© ì„ ì§€ (ì—¬ëŸ¬ ê°œì˜ ë‹¨ì¼ ì„ ì§€ë¥¼ í¬í•¨í•˜ëŠ” ì„ ì§€)ê°€ ìˆëŠ” ë¬¸ì œì˜ ê²½ìš°, ì¡°í•©ì„ ì§€ë¿ ì•„ë‹ˆë¼ ë‹¨ì¼ ì„ ì§€ë„ ì •ë‹µìœ¼ë¡œ ê³ ë¥¼ ìˆ˜ ìˆìŒì„ ëª…ì‹¬í•  ê²ƒ. 
    - ì •ë‹µì´ ì—†ëŠ” ê²½ìš°ëŠ” ì—†ìœ¼ë©°, ì˜¤ì§ í•˜ë‚˜ì˜ ì •ë‹µë§Œ ê³ ë¥¼ ê²ƒ.
    - ì§ˆë¬¸ê³¼ ì„ ì§€ì— ìˆëŠ” ìˆ«ì, ë‹¨ì–´ ë“±ì— ì˜¤íƒ€ê°€ ìˆë‹¤ê³  ìƒê°í•˜ì§€ ë§ ê²ƒ.
    - ë§Œì•½ í™•ì‹¤í•œ ë‹µì´ ì—†ì–´ë„, ë°˜ë“œì‹œ ì •ë‹µì— ê°€ì¥ ê·¼ì ‘í•œ ì„ ì§€ (ì˜ˆì‹œ: ì •ë‹µì˜ ë‚´ìš©ì„ ê°€ì¥ ë§ì´ í¬í•¨í•œ ì„ ì§€, ì •ë‹µ ê·¸ ìì²´ì¸ ì„ ì§€)ë¥¼ ê³ ë¥´ê³  ê·¸ ì´ìœ ë¥¼ ë…¼ë¦¬ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•  ê²ƒ.
    - ë‹µ ì¶œë ¥: ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ì„ ë”°ë¼ì•¼ í•¨:
     [ANSWER]: (X) í…ìŠ¤íŠ¸
     
    **ì‹¤í–‰ ë‹¨ê³„** 
    [1ë‹¨ê³„]: ë¬¸ì œë¥¼ ìì„¸í•˜ê²Œ ì½ê³  ì§ˆë¬¸ì˜ ë‚´ìš©ê³¼ ì˜ë„ë¥¼ ëª…í™•í•˜ê²Œ ì´í•´.
    [2ë‹¨ê³„]: ë¬¸ì œì˜ ê° ì„ ì§€ê°€ ì •ë‹µ ë˜ëŠ” ì˜¤ë‹µì¸ ì´ìœ ë¥¼ ì•„ë˜ ë‘ ê°€ì§€ ê³ ë ¤ ì‚¬í•­ì— ëŒ€í•´ ë§¤ìš° ìƒì„¸í•˜ê²Œ í‰ê°€í•˜ê³  ì„¤ëª….
      - ë§¥ë½ì— ì£¼ì–´ì§„ ì •ë³´ì™€ì˜ ì¼ì¹˜ ì—¬ë¶€: ì„ ì§€ì˜ ë‚´ìš©ì´ ë§¥ë½ì˜ ì •ë³´ì™€ ëª¨ìˆœë˜ëŠ”ì§€ í™•ì¸
      - ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œì„œì˜ ì ì ˆì„±: ì„ ì§€ì˜ ë‚´ìš©ì´ ì§ˆë¬¸ì˜ ë‚´ìš©ê³¼ ì˜ë„ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    [3ë‹¨ê³„]: ê²€ì¦
      - ìµœì¢… ë‹µì„ ë°˜í™˜í•˜ê¸° ì „, ìŠ¤ìŠ¤ë¡œì—ê²Œ ì§ˆë¬¸: "ì´ ë‹µì´ í™•ì‹¤í•œê°€?"
    [4ë‹¨ê³„]: ìµœì¢… ë‹µì´ ì‹¤í–‰ í”„ë¡œí† ì½œì˜ ê° í•­ëª©ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê¼¼ê¼¼íˆ ì ê²€
    [5ë‹¨ê³„]: ì‹¤í–‰ í”„ë¡œí† ì½œì— ì •ì˜ëœ ë‹µ ì¶œë ¥ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ ì •ë‹µ ë°˜í™˜
    """),

    ("human",
    """
    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë§ì¶° ìµœì¢… ë‹µê³¼ í•¨ê»˜ ê°ê°ì˜ ë¬¸ì œ í’€ì´ ê³¼ì •ê¹Œì§€ êµ¬ì²´ì ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

    ë¬¸ì œ:
    {question}

    ë§¥ë½:
    {context}
    """)
])

    # RAG chain
    rag_chain = prompt_template | llm

    # call RAG chain
    response = rag_chain.invoke({"question": prompt, "context": context})
    answer = response.content

    print(f"ğŸ’¬ answer: {answer}")

    return answer

def mmlu_law_rag(prompt, context, llm):

    # ---- Prompt Template ----
    prompt_template = ChatPromptTemplate.from_messages([
        ("system",
"""
You are a legal expert who clearly understands the situation, identifies key points of dispute, applies relevant legal provisions, and accurately determines each partyâ€™s responsibility.

Carefully familiarize yourself with the execution protocol. 
Follow every step in order exactly and produce the correct final answer. Explain the process of each step very concretely and logically.

**Execution Protocol**:
- Do not output the answer at the beginning; complete all execution steps first, then provide the correct final answer.
- There is always an answer; choose exactly one answer.
- Output format: the output must follow this exact format:
	[ANSWER]: (X) text

**Execution Steps**:
[Step 1]: Read the question carefully and clearly understand the content and intent of the question.
[Step 2]: For each answer choice, evaluate and explain in great detail whether it is correct or incorrect with respect to the following two considerations:
    - Consistency with information given in the context or your internal knowledge: check whether the choice contradicts the context or your knowledge.
    - Appropriateness as an answer to the question: check whether the choice matches the content and intent of the question.
[Step 3]: Verification
    - Before returning the final answer, ask yourself: "Is this answer certain?â€
[Step 4]: Return the correct answer exactly following the answer output format defined in the execution protocol.
"""),
        ("human",
"""
    Following the system prompt, output the final answer along with a detailed, step-by-step solution process for each question.

    question:
    {question}

    context:
    {context}
""")
    ])

    rag_chain = prompt_template | llm

    # ---- Call RAG chain ----
    response = rag_chain.invoke({
        "question": prompt,
        "context": context
    })

    answer = response.content
    print(f"ğŸ’¬ answer: {answer}")

    return answer

def mmlu_psychology_rag(prompt, context, llm):

    # ---- Prompt Template ----
    prompt_template = ChatPromptTemplate.from_messages([
        ("system",
"""
You are a psychology expert who accurately understands human behavior and mental processes and identifies key psychological factors in a given situation.
You can provide clear, evidence-based analysis grounded in established psychological theories and research.

Carefully familiarize yourself with the execution protocol. 
Follow every step in order exactly and produce the correct final answer. Explain the process of each step very concretely and logically.

**Execution Protocol**:
- Do not output the answer at the beginning; complete all execution steps first, then provide the correct final answer.
- There is always an answer; choose exactly one answer.
- Output format: the output must follow this exact format:
	[ANSWER]: (X) text

**Execution Steps**:
[Step 1]: Read the question carefully and clearly understand the content and intent of the question.
[Step 2]: For each answer choice, evaluate and explain in great detail whether it is correct or incorrect with respect to the following two considerations:
    - Consistency with information given in the context or your internal knowledge: check whether the choice contradicts the context or your knowledge.
    - Appropriateness as an answer to the question: check whether the choice matches the content and intent of the question.
[Step 3]: Verification
    - Before returning the final answer, ask yourself: "Is this answer certain?â€
[Step 4]: Return the correct answer exactly following the answer output format defined in the execution protocol.
"""),
        ("human",
"""
    Following the system prompt, output the final answer along with a detailed, step-by-step solution process for each question.

    question:
    {question}

    context:
    {context}
""")
    ])

    rag_chain = prompt_template | llm

    # ---- Call RAG chain ----
    response = rag_chain.invoke({
        "question": prompt,
        "context": context
    })

    answer = response.content
    print(f"ğŸ’¬ answer: {answer}")

    return answer

def mmlu_philosophy_rag(prompt, context, llm):

    # ---- Prompt Template ----
    prompt_template = ChatPromptTemplate.from_messages([
        ("system",
"""
You are a philosophy expert who clearly analyzes abstract concepts and arguments and identifies underlying assumptions and logical structure.
You can evaluate positions using established philosophical theories and rigorous reasoning.

Carefully familiarize yourself with the execution protocol. 
Follow every step in order exactly and produce the correct final answer. Explain the process of each step very concretely and logically.

**Execution Protocol**:
- Do not output the answer at the beginning; complete all execution steps first, then provide the correct final answer.
- There is always an answer; choose exactly one answer.
- Output format: the output must follow this exact format:
	[ANSWER]: (X) text

**Execution Steps**:
[Step 1]: Read the question carefully and clearly understand the content and intent of the question.
[Step 2]: For each answer choice, evaluate and explain in great detail whether it is correct or incorrect with respect to the following two considerations:
    - Consistency with information given in the context or your internal knowledge: check whether the choice contradicts the context or your knowledge.
    - Appropriateness as an answer to the question: check whether the choice matches the content and intent of the question.
[Step 3]: Verification
    - Before returning the final answer, ask yourself: "Is this answer certain?â€
[Step 4]: Return the correct answer exactly following the answer output format defined in the execution protocol.
"""),
        ("human",
"""
    Following the system prompt, output the final answer along with a detailed, step-by-step solution process for each question.

    question:
    {question}

    context:
    {context}
""")
    ])

    rag_chain = prompt_template | llm

    # ---- Call RAG chain ----
    response = rag_chain.invoke({
        "question": prompt,
        "context": context
    })

    answer = response.content
    print(f"ğŸ’¬ answer: {answer}")

    return answer

def mmlu_history_rag(prompt, context, llm):

    # ---- Prompt Template ----
    prompt_template = ChatPromptTemplate.from_messages([
        ("system",
"""
You are a history expert who accurately understands historical contexts and identifies key events and causal relationships. 
You can evaluate sources critically and provides well-reasoned interpretations grounded in historical evidence.

Carefully familiarize yourself with the execution protocol. 
Follow every step in order exactly and produce the correct final answer. Explain the process of each step very concretely and logically.

**Execution Protocol**:
- Do not output the answer at the beginning; complete all execution steps first, then provide the correct final answer.
- There is always an answer; choose exactly one answer.
- Output format: the output must follow this exact format:
	[ANSWER]: (X) text

**Execution Steps**:
[Step 1]: Read the question carefully and clearly understand the content and intent of the question.
[Step 2]: For each answer choice, evaluate and explain in great detail whether it is correct or incorrect with respect to the following two considerations:
    - Consistency with information given in the context or your internal knowledge: check whether the choice contradicts the context or your knowledge.
    - Appropriateness as an answer to the question: check whether the choice matches the content and intent of the question.
[Step 3]: Verification
    - Before returning the final answer, ask yourself: "Is this answer certain?â€
[Step 4]: Return the correct answer exactly following the answer output format defined in the execution protocol.
"""),
        ("human",
"""
    Following the system prompt, output the final answer along with a detailed, step-by-step solution process for each question.

    question:
    {question}

    context:
    {context}
""")
    ])

    rag_chain = prompt_template | llm

    # ---- Call RAG chain ----
    response = rag_chain.invoke({
        "question": prompt,
        "context": context
    })

    answer = response.content
    print(f"ğŸ’¬ answer: {answer}")

    return answer

def mmlu_business_rag(prompt, context, llm):

    # ---- Prompt Template ----
    prompt_template = ChatPromptTemplate.from_messages([
        ("system",
"""
You are a business expert who understands organizational and market dynamics and identifies strategic issues and opportunities. 
You can apply relevant business frameworks and provides clear, practical analysis to support sound decision-making.

Carefully familiarize yourself with the execution protocol. 
Follow every step in order exactly and produce the correct final answer. Explain the process of each step very concretely and logically.

**Execution Protocol**:
- Do not output the answer at the beginning; complete all execution steps first, then provide the correct final answer.
- There is always an answer; choose exactly one answer.
- Output format: the output must follow this exact format:
	[ANSWER]: (X) text

**Execution Steps**:
[Step 1]: Read the question carefully and clearly understand the content and intent of the question.
[Step 2]: For each answer choice, evaluate and explain in great detail whether it is correct or incorrect with respect to the following two considerations:
    - Consistency with information given in the context or your internal knowledge: check whether the choice contradicts the context or your knowledge.
    - Appropriateness as an answer to the question: check whether the choice matches the content and intent of the question.
[Step 3]: Verification
    - Before returning the final answer, ask yourself: "Is this answer certain?â€
[Step 4]: Return the correct answer exactly following the answer output format defined in the execution protocol.
"""),
        ("human",
"""
    Following the system prompt, output the final answer along with a detailed, step-by-step solution process for each question.

    question:
    {question}

    context:
    {context}
""")
    ])

    rag_chain = prompt_template | llm

    # ---- Call RAG chain ----
    response = rag_chain.invoke({
        "question": prompt,
        "context": context
    })

    answer = response.content
    print(f"ğŸ’¬ answer: {answer}")

    return answer

